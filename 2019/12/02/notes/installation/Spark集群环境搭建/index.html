<!DOCTYPE html>
<html>
  <!-- Html Head Tag-->
  <head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="Piece One">
  <!-- Open Graph Data -->
  <meta property="og:title" content="notes/installation/Spark集群环境搭建"/>
  <meta property="og:description" content="爱吃鱼的Blog" />
  <meta property="og:site_name" content="TLUYZ&#39;Blog"/>
  <meta property="og:type" content="article" />
  <meta property="og:image" content="http://yoursite.com"/>
  
    <link rel="alternate" href="/atom.xml" title="TLUYZ&#39;Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  

  <!-- Site Title -->
  <title>TLUYZ'Blog</title>

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <!-- Custom CSS -->
  
  <link rel="stylesheet" href="/css/style.light.css">

  <!-- Google Analytics -->
  

</head>

  <body>
    <!-- Page Header -->


<header class="site-header header-background" style="background-image: url(/img/default-banner-dark.jpg)">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="page-title with-background-image">
          <p class="title">notes/installation/Spark集群环境搭建</p>
          <p class="subtitle"></p>
        </div>
        <div class="site-menu with-background-image">
          <ul>
            
              <li>
                <a href="/">
                  
                  Home
                  
                </a>
              </li>
            
              <li>
                <a href="/archives">
                  
                  Archives
                  
                </a>
              </li>
            
              <li>
                <a href="https://github.com/xiaobaigitb" target="_blank" rel="noopener">
                  
                  Github
                  
                </a>
              </li>
            
              <li>
                <a href="mailto:<your-email-address>">
                  
                  Email
                  
                </a>
              </li>
            
          </ul>
        </div>
      </div>
    </div>
  </div>
</header>

<article>
  <div class="container typo">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-info text-muted">
          
            <!-- Author -->
            <span class="author info">By Piece One</span>
          
          <!-- Date -->
          <span class="date-time info">On
            <span class="date">2019-12-02</span>
            <span class="time">22:05:46</span>
          </span>
          
        </div>
        <!-- Tags -->
        
        <!-- Post Main Content -->
        <div class="post-content">
          <h1 id="基于ZooKeeper搭建Spark高可用集群"><a href="#基于ZooKeeper搭建Spark高可用集群" class="headerlink" title="基于ZooKeeper搭建Spark高可用集群"></a>基于ZooKeeper搭建Spark高可用集群</h1><nav>
<a href="#一集群规划">一、集群规划</a><br/>
<a href="#二前置条件">二、前置条件</a><br/>
<a href="#三Spark集群搭建">三、Spark集群搭建</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#31-下载解压">3.1 下载解压</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#32-配置环境变量">3.2 配置环境变量</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#33-集群配置">3.3 集群配置</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#34-安装包分发">3.4 安装包分发</a><br/>
<a href="#四启动集群">四、启动集群</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#41-启动ZooKeeper集群">4.1 启动ZooKeeper集群</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#42-启动Hadoop集群">4.2 启动Hadoop集群</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#43-启动Spark集群">4.3 启动Spark集群</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#44-查看服务">4.4 查看服务</a><br/>
<a href="#五验证集群高可用">五、验证集群高可用</a><br/>
<a href="#六提交作业">六、提交作业</a><br/>
</nav>


<h2 id="一、集群规划"><a href="#一、集群规划" class="headerlink" title="一、集群规划"></a>一、集群规划</h2><p>这里搭建一个 3 节点的 Spark 集群，其中三台主机上均部署 <code>Worker</code> 服务。同时为了保证高可用，除了在 hadoop001 上部署主 <code>Master</code> 服务外，还在 hadoop002 和 hadoop003 上分别部署备用的 <code>Master</code> 服务，Master 服务由 Zookeeper 集群进行协调管理，如果主 <code>Master</code> 不可用，则备用 <code>Master</code> 会成为新的主 <code>Master</code>。</p>
<div align="center"> <img  src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/spark集群规划.png"/> </div>

<h2 id="二、前置条件"><a href="#二、前置条件" class="headerlink" title="二、前置条件"></a>二、前置条件</h2><p>搭建 Spark 集群前，需要保证 JDK 环境、Zookeeper 集群和 Hadoop 集群已经搭建，相关步骤可以参阅：</p>
<ul>
<li><a href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/installation/Linux下JDK安装.md" target="_blank" rel="noopener">Linux 环境下 JDK 安装</a></li>
<li><a href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/installation/Zookeeper单机环境和集群环境搭建.md" target="_blank" rel="noopener">Zookeeper 单机环境和集群环境搭建</a></li>
<li><a href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/installation/Hadoop集群环境搭建.md" target="_blank" rel="noopener">Hadoop 集群环境搭建</a></li>
</ul>
<h2 id="三、Spark集群搭建"><a href="#三、Spark集群搭建" class="headerlink" title="三、Spark集群搭建"></a>三、Spark集群搭建</h2><h3 id="3-1-下载解压"><a href="#3-1-下载解压" class="headerlink" title="3.1 下载解压"></a>3.1 下载解压</h3><p>下载所需版本的 Spark，官网下载地址：<a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">http://spark.apache.org/downloads.html</a></p>
<div align="center"> <img width="600px" src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/spark-download.png"/> </div>



<p>下载后进行解压：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> tar -zxvf  spark-2.2.3-bin-hadoop2.6.tgz</span></span></pre></td></tr></table></figure>



<h3 id="3-2-配置环境变量"><a href="#3-2-配置环境变量" class="headerlink" title="3.2 配置环境变量"></a>3.2 配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/profile</span></span></pre></td></tr></table></figure>

<p>添加环境变量：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">export SPARK_HOME=/usr/app/spark-2.2.3-bin-hadoop2.6</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">export  PATH=$&#123;SPARK_HOME&#125;/bin:$PATH</span></pre></td></tr></table></figure>

<p>使得配置的环境变量立即生效：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> /etc/profile</span></span></pre></td></tr></table></figure>

<h3 id="3-3-集群配置"><a href="#3-3-集群配置" class="headerlink" title="3.3 集群配置"></a>3.3 集群配置</h3><p>进入 <code>${SPARK_HOME}/conf</code> 目录，拷贝配置样本进行修改：</p>
<h4 id="1-spark-env-sh"><a href="#1-spark-env-sh" class="headerlink" title="1. spark-env.sh"></a>1. spark-env.sh</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">cp spark-env.sh.template spark-env.sh</span></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置JDK安装位置</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/usr/java/jdk1.8.0_201</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置hadoop配置文件的位置</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">HADOOP_CONF_DIR=/usr/app/hadoop-2.6.0-cdh5.15.2/etc/hadoop</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置zookeeper地址</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop001:2181,hadoop002:2181,hadoop003:2181 -Dspark.deploy.zookeeper.dir=/spark"</span></pre></td></tr></table></figure>

<h4 id="2-slaves"><a href="#2-slaves" class="headerlink" title="2. slaves"></a>2. slaves</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">cp slaves.template slaves</span></pre></td></tr></table></figure>

<p>配置所有 Woker 节点的位置：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="attr">hadoop001</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="attr">hadoop002</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="attr">hadoop003</span></span></pre></td></tr></table></figure>

<h3 id="3-4-安装包分发"><a href="#3-4-安装包分发" class="headerlink" title="3.4 安装包分发"></a>3.4 安装包分发</h3><p>将 Spark 的安装包分发到其他服务器，分发后建议在这两台服务器上也配置一下 Spark 的环境变量。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">scp -r /usr/app/spark-2.4.0-bin-hadoop2.6/   hadoop002:usr/app/</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">scp -r /usr/app/spark-2.4.0-bin-hadoop2.6/   hadoop003:usr/app/</span></pre></td></tr></table></figure>



<h2 id="四、启动集群"><a href="#四、启动集群" class="headerlink" title="四、启动集群"></a>四、启动集群</h2><h3 id="4-1-启动ZooKeeper集群"><a href="#4-1-启动ZooKeeper集群" class="headerlink" title="4.1 启动ZooKeeper集群"></a>4.1 启动ZooKeeper集群</h3><p>分别到三台服务器上启动 ZooKeeper 服务：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span></pre></td></tr></table></figure>

<h3 id="4-2-启动Hadoop集群"><a href="#4-2-启动Hadoop集群" class="headerlink" title="4.2 启动Hadoop集群"></a>4.2 启动Hadoop集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动dfs服务</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动yarn服务</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">start-yarn.sh</span></pre></td></tr></table></figure>

<h3 id="4-3-启动Spark集群"><a href="#4-3-启动Spark集群" class="headerlink" title="4.3 启动Spark集群"></a>4.3 启动Spark集群</h3><p>进入 hadoop001 的 <code>${SPARK_HOME}/sbin</code> 目录下，执行下面命令启动集群。执行命令后，会在 hadoop001 上启动 <code>Maser</code> 服务，会在 <code>slaves</code> 配置文件中配置的所有节点上启动 <code>Worker</code> 服务。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">start-all.sh</span></pre></td></tr></table></figure>

<p>分别在 hadoop002 和 hadoop003 上执行下面的命令，启动备用的 <code>Master</code> 服务：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin 下执行</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">start-master.sh</span></pre></td></tr></table></figure>

<h3 id="4-4-查看服务"><a href="#4-4-查看服务" class="headerlink" title="4.4 查看服务"></a>4.4 查看服务</h3><p>查看 Spark 的 Web-UI 页面，端口为 <code>8080</code>。此时可以看到 hadoop001 上的 Master 节点处于 <code>ALIVE</code> 状态，并有 3 个可用的 <code>Worker</code> 节点。</p>
<div align="center"> <img  src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/spark-集群搭建1.png"/> </div>

<p>而 hadoop002 和 hadoop003 上的 Master 节点均处于 <code>STANDBY</code> 状态，没有可用的 <code>Worker</code> 节点。</p>
<div align="center"> <img  src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/spark-集群搭建2.png"/> </div>

<div align="center"> <img  src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/spark-集群搭建3.png"/> </div>



<h2 id="五、验证集群高可用"><a href="#五、验证集群高可用" class="headerlink" title="五、验证集群高可用"></a>五、验证集群高可用</h2><p>此时可以使用 <code>kill</code> 命令杀死 hadoop001 上的 <code>Master</code> 进程，此时备用 <code>Master</code> 会中会有一个再次成为 <code>主 Master</code>，我这里是 hadoop002，可以看到 hadoop2 上的 <code>Master</code> 经过 <code>RECOVERING</code> 后成为了新的主 <code>Master</code>，并且获得了全部可以用的 <code>Workers</code>。</p>
<div align="center"> <img  src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/spark-集群搭建4.png"/> </div>

<p>Hadoop002 上的 <code>Master</code> 成为主 <code>Master</code>，并获得了全部可以用的 <code>Workers</code>。</p>
<div align="center"> <img  src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/spark-集群搭建5.png"/> </div>

<p>此时如果你再在 hadoop001 上使用 <code>start-master.sh</code> 启动 Master 服务，那么其会作为备用 <code>Master</code> 存在。</p>
<h2 id="六、提交作业"><a href="#六、提交作业" class="headerlink" title="六、提交作业"></a>六、提交作业</h2><p>和单机环境下的提交到 Yarn 上的命令完全一致，这里以 Spark 内置的计算 Pi 的样例程序为例，提交命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">spark-submit \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--class org.apache.spark.examples.SparkPi \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--master yarn \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--deploy-mode client \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--executor-memory 1G \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--num-executors 10 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">100</span></pre></td></tr></table></figure>


        </div>
      </div>
    </div>
  </div>
</article>



    <!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <p class="copyright text-muted">
          Theme By <a target="_blank" href="https://github.com/levblanc">Levblanc.</a>
          Inspired By <a target="_blank" href="https://github.com/klugjo/hexo-theme-clean-blog">Clean Blog.</a>
        <p class="copyright text-muted">
          Powered By <a target="_blank" href="https://hexo.io/">Hexo.</a>
        </p>
      </div>
    </div>
  </div>
</footer>


    <!-- After Footer Scripts -->
<script src="/js/highlight.pack.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function(event) {
    var codeBlocks = Array.prototype.slice.call(document.getElementsByTagName('pre'))
    codeBlocks.forEach(function(block, index) {
      hljs.highlightBlock(block);
    });
  });
</script>

  </body>
</html>

