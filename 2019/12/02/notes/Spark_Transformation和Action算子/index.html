<!DOCTYPE html>
<html>
  <!-- Html Head Tag-->
  <head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="Piece One">
  <!-- Open Graph Data -->
  <meta property="og:title" content="notes/Spark_Transformation和Action算子"/>
  <meta property="og:description" content="爱吃鱼的Blog" />
  <meta property="og:site_name" content="TLUYZ&#39;Blog"/>
  <meta property="og:type" content="article" />
  <meta property="og:image" content="http://yoursite.com"/>
  
    <link rel="alternate" href="/atom.xml" title="TLUYZ&#39;Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  

  <!-- Site Title -->
  <title>TLUYZ'Blog</title>

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <!-- Custom CSS -->
  
  <link rel="stylesheet" href="/css/style.light.css">

  <!-- Google Analytics -->
  

</head>

  <body>
    <!-- Page Header -->


<header class="site-header header-background" style="background-image: url(/img/default-banner-dark.jpg)">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="page-title with-background-image">
          <p class="title">notes/Spark_Transformation和Action算子</p>
          <p class="subtitle"></p>
        </div>
        <div class="site-menu with-background-image">
          <ul>
            
              <li>
                <a href="/">
                  
                  Home
                  
                </a>
              </li>
            
              <li>
                <a href="/archives">
                  
                  Archives
                  
                </a>
              </li>
            
              <li>
                <a href="https://github.com/xiaobaigitb" target="_blank" rel="noopener">
                  
                  Github
                  
                </a>
              </li>
            
              <li>
                <a href="mailto:<your-email-address>">
                  
                  Email
                  
                </a>
              </li>
            
          </ul>
        </div>
      </div>
    </div>
  </div>
</header>

<article>
  <div class="container typo">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-info text-muted">
          
            <!-- Author -->
            <span class="author info">By Piece One</span>
          
          <!-- Date -->
          <span class="date-time info">On
            <span class="date">2019-12-02</span>
            <span class="time">22:05:46</span>
          </span>
          
        </div>
        <!-- Tags -->
        
        <!-- Post Main Content -->
        <div class="post-content">
          <h1 id="Transformation-和-Action-常用算子"><a href="#Transformation-和-Action-常用算子" class="headerlink" title="Transformation 和 Action 常用算子"></a>Transformation 和 Action 常用算子</h1><nav>
<a href="#一Transformation">一、Transformation</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#11-map">1.1 map</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#12-filter">1.2 filter</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#13-flatMap">1.3 flatMap</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#14-mapPartitions">1.4 mapPartitions</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#15-mapPartitionsWithIndex">1.5 mapPartitionsWithIndex</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#16-sample">1.6 sample</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#17-union">1.7 union</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#18-intersection">1.8 intersection</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#19-distinct">1.9 distinct</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#110-groupByKey">1.10 groupByKey</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#111-reduceByKey">1.11 reduceByKey</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#112-sortBy--sortByKey">1.12 sortBy & sortByKey </a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#113-join">1.13 join</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#114-cogroup">1.14 cogroup</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#115-cartesian">1.15 cartesian</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#116-aggregateByKey">1.16 aggregateByKey</a><br/>
<a href="#二Action">二、Action</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#21-reduce">2.1 reduce</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#22-takeOrdered">2.2 takeOrdered</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#23-countByKey">2.3 countByKey</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#24-saveAsTextFile">2.4 saveAsTextFile</a><br/>
</nav>

<h2 id="一、Transformation"><a href="#一、Transformation" class="headerlink" title="一、Transformation"></a>一、Transformation</h2><p>spark 常用的 Transformation 算子如下表：</p>
<table>
<thead>
<tr>
<th>Transformation 算子</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody><tr>
<td><strong>map</strong>(<em>func</em>)</td>
<td>对原 RDD 中每个元素运用 <em>func</em> 函数，并生成新的 RDD</td>
</tr>
<tr>
<td><strong>filter</strong>(<em>func</em>)</td>
<td>对原 RDD 中每个元素使用<em>func</em> 函数进行过滤，并生成新的 RDD</td>
</tr>
<tr>
<td><strong>flatMap</strong>(<em>func</em>)</td>
<td>与 map 类似，但是每一个输入的 item 被映射成 0 个或多个输出的 items（ <em>func</em> 返回类型需要为 Seq ）。</td>
</tr>
<tr>
<td><strong>mapPartitions</strong>(<em>func</em>)</td>
<td>与 map 类似，但函数单独在 RDD 的每个分区上运行， <em>func</em>函数的类型为  Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; ，其中 T 是 RDD 的类型，即 RDD[T]</td>
</tr>
<tr>
<td><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td>
<td>与 mapPartitions 类似，但 <em>func</em> 类型为 (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; ，其中第一个参数为分区索引</td>
</tr>
<tr>
<td><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td>
<td>数据采样，有三个可选参数：设置是否放回（withReplacement）、采样的百分比（<em>fraction</em>）、随机数生成器的种子（seed）；</td>
</tr>
<tr>
<td><strong>union</strong>(<em>otherDataset</em>)</td>
<td>合并两个 RDD</td>
</tr>
<tr>
<td><strong>intersection</strong>(<em>otherDataset</em>)</td>
<td>求两个 RDD 的交集</td>
</tr>
<tr>
<td><strong>distinct</strong>([<em>numTasks</em>]))</td>
<td>去重</td>
</tr>
<tr>
<td><strong>groupByKey</strong>([<em>numTasks</em>])</td>
<td>按照 key 值进行分区，即在一个 (K, V) 对的 dataset 上调用时，返回一个 (K, Iterable&lt;V&gt;) <br/><strong>Note:</strong> 如果分组是为了在每一个 key 上执行聚合操作（例如，sum 或 average)，此时使用 <code>reduceByKey</code> 或 <code>aggregateByKey</code> 性能会更好<br><strong>Note:</strong> 默认情况下，并行度取决于父 RDD 的分区数。可以传入 <code>numTasks</code> 参数进行修改。</td>
</tr>
<tr>
<td><strong>reduceByKey</strong>(<em>func</em>, [<em>numTasks</em>])</td>
<td>按照 key 值进行分组，并对分组后的数据执行归约操作。</td>
</tr>
<tr>
<td><strong>aggregateByKey</strong>(<em>zeroValue</em>,<em>numPartitions</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numTasks</em>])</td>
<td>当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和 zeroValue 聚合每个键的值。与 groupByKey 类似，reduce 任务的数量可通过第二个参数进行配置。</td>
</tr>
<tr>
<td><strong>sortByKey</strong>([<em>ascending</em>], [<em>numTasks</em>])</td>
<td>按照 key 进行排序，其中的 key 需要实现 Ordered 特质，即可比较</td>
</tr>
<tr>
<td><strong>join</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td>
<td>在一个 (K, V) 和 (K, W) 类型的 dataset 上调用时，返回一个 (K, (V, W)) pairs 的 dataset，等价于内连接操作。如果想要执行外连接，可以使用 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 等算子。</td>
</tr>
<tr>
<td><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td>
<td>在一个 (K, V) 对的 dataset 上调用时，返回一个 (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) tuples 的 dataset。</td>
</tr>
<tr>
<td><strong>cartesian</strong>(<em>otherDataset</em>)</td>
<td>在一个 T 和 U 类型的 dataset 上调用时，返回一个 (T, U) 类型的 dataset（即笛卡尔积）。</td>
</tr>
<tr>
<td><strong>coalesce</strong>(<em>numPartitions</em>)</td>
<td>将 RDD 中的分区数减少为 numPartitions。</td>
</tr>
<tr>
<td><strong>repartition</strong>(<em>numPartitions</em>)</td>
<td>随机重新调整 RDD 中的数据以创建更多或更少的分区，并在它们之间进行平衡。</td>
</tr>
<tr>
<td><strong>repartitionAndSortWithinPartitions</strong>(<em>partitioner</em>)</td>
<td>根据给定的 partitioner（分区器）对 RDD 进行重新分区，并对分区中的数据按照 key 值进行排序。这比调用 <code>repartition</code> 然后再 sorting（排序）效率更高，因为它可以将排序过程推送到 shuffle 操作所在的机器。</td>
</tr>
</tbody></table>
<p>下面分别给出这些算子的基本使用示例：</p>
<h3 id="1-1-map"><a href="#1-1-map" class="headerlink" title="1.1 map"></a>1.1 map</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list).map(_ * <span class="number">10</span>).foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出结果： 10 20 30 （这里为了节省篇幅去掉了换行,后文亦同）</span></span></pre></td></tr></table></figure>

<h3 id="1-2-filter"><a href="#1-2-filter" class="headerlink" title="1.2 filter"></a>1.2 filter</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">21</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list).filter(_ &gt;= <span class="number">10</span>).foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出： 10 12 21</span></span></pre></td></tr></table></figure>

<h3 id="1-3-flatMap"><a href="#1-3-flatMap" class="headerlink" title="1.3 flatMap"></a>1.3 flatMap</h3><p><code>flatMap(func)</code> 与 <code>map</code> 类似，但每一个输入的 item 会被映射成 0 个或多个输出的 items（ <em>func</em> 返回类型需要为 <code>Seq</code>）。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>), <span class="type">List</span>(<span class="number">3</span>), <span class="type">List</span>(), <span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list).flatMap(_.toList).map(_ * <span class="number">10</span>).foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出结果 ： 10 20 30 40 50</span></span></pre></td></tr></table></figure>

<p>flatMap 这个算子在日志分析中使用概率非常高，这里进行一下演示：拆分输入的每行数据为单个单词，并赋值为 1，代表出现一次，之后按照单词分组并统计其出现总次数，代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lines = <span class="type">List</span>(<span class="string">"spark flume spark"</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">                 <span class="string">"hadoop flume hive"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(lines).flatMap(line =&gt; line.split(<span class="string">" "</span>)).</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">map(word=&gt;(word,<span class="number">1</span>)).reduceByKey(_+_).foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">(spark,<span class="number">2</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">(hive,<span class="number">1</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">(hadoop,<span class="number">1</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">(flume,<span class="number">2</span>)</span></pre></td></tr></table></figure>

<h3 id="1-4-mapPartitions"><a href="#1-4-mapPartitions" class="headerlink" title="1.4 mapPartitions"></a>1.4 mapPartitions</h3><p>与 map 类似，但函数单独在 RDD 的每个分区上运行， <em>func</em>函数的类型为 <code>Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;</code> (其中 T 是 RDD 的类型)，即输入和输出都必须是可迭代类型。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list, <span class="number">3</span>).mapPartitions(iterator =&gt; &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">Int</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">while</span> (iterator.hasNext) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    buffer.append(iterator.next() * <span class="number">100</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">  buffer.toIterator</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">&#125;).foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//输出结果</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="number">100</span> <span class="number">200</span> <span class="number">300</span> <span class="number">400</span> <span class="number">500</span> <span class="number">600</span></span></pre></td></tr></table></figure>

<h3 id="1-5-mapPartitionsWithIndex"><a href="#1-5-mapPartitionsWithIndex" class="headerlink" title="1.5 mapPartitionsWithIndex"></a>1.5 mapPartitionsWithIndex</h3><p>  与 mapPartitions 类似，但 <em>func</em> 类型为 <code>(Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt;</code> ，其中第一个参数为分区索引。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list, <span class="number">3</span>).mapPartitionsWithIndex((index, iterator) =&gt; &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">String</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  <span class="keyword">while</span> (iterator.hasNext) &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    buffer.append(index + <span class="string">"分区:"</span> + iterator.next() * <span class="number">100</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">  buffer.toIterator</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">&#125;).foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//输出</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="number">0</span> 分区:<span class="number">100</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="number">0</span> 分区:<span class="number">200</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> 分区:<span class="number">300</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> 分区:<span class="number">400</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="number">2</span> 分区:<span class="number">500</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="number">2</span> 分区:<span class="number">600</span></span></pre></td></tr></table></figure>

<h3 id="1-6-sample"><a href="#1-6-sample" class="headerlink" title="1.6 sample"></a>1.6 sample</h3><p>  数据采样。有三个可选参数：设置是否放回 (withReplacement)、采样的百分比 (fraction)、随机数生成器的种子 (seed) ：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list).sample(withReplacement = <span class="literal">false</span>, fraction = <span class="number">0.5</span>).foreach(println)</span></pre></td></tr></table></figure>

<h3 id="1-7-union"><a href="#1-7-union" class="headerlink" title="1.7 union"></a>1.7 union</h3><p>合并两个 RDD：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list1).union(sc.parallelize(list2)).foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出: 1 2 3 4 5 6</span></span></pre></td></tr></table></figure>

<h3 id="1-8-intersection"><a href="#1-8-intersection" class="headerlink" title="1.8 intersection"></a>1.8 intersection</h3><p>求两个 RDD 的交集：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list1).intersection(sc.parallelize(list2)).foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出:  4 5</span></span></pre></td></tr></table></figure>

<h3 id="1-9-distinct"><a href="#1-9-distinct" class="headerlink" title="1.9 distinct"></a>1.9 distinct</h3><p>去重：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list).distinct().foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出: 4 1 2</span></span></pre></td></tr></table></figure>

<h3 id="1-10-groupByKey"><a href="#1-10-groupByKey" class="headerlink" title="1.10 groupByKey"></a>1.10 groupByKey</h3><p>按照键进行分组：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">"hadoop"</span>, <span class="number">2</span>), (<span class="string">"spark"</span>, <span class="number">3</span>), (<span class="string">"spark"</span>, <span class="number">5</span>), (<span class="string">"storm"</span>, <span class="number">6</span>), (<span class="string">"hadoop"</span>, <span class="number">2</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list).groupByKey().map(x =&gt; (x._1, x._2.toList)).foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//输出：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">(spark,<span class="type">List</span>(<span class="number">3</span>, <span class="number">5</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">(hadoop,<span class="type">List</span>(<span class="number">2</span>, <span class="number">2</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">(storm,<span class="type">List</span>(<span class="number">6</span>))</span></pre></td></tr></table></figure>

<h3 id="1-11-reduceByKey"><a href="#1-11-reduceByKey" class="headerlink" title="1.11 reduceByKey"></a>1.11 reduceByKey</h3><p>按照键进行归约操作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">"hadoop"</span>, <span class="number">2</span>), (<span class="string">"spark"</span>, <span class="number">3</span>), (<span class="string">"spark"</span>, <span class="number">5</span>), (<span class="string">"storm"</span>, <span class="number">6</span>), (<span class="string">"hadoop"</span>, <span class="number">2</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list).reduceByKey(_ + _).foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//输出</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">(spark,<span class="number">8</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">(hadoop,<span class="number">4</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">(storm,<span class="number">6</span>)</span></pre></td></tr></table></figure>

<h3 id="1-12-sortBy-amp-sortByKey"><a href="#1-12-sortBy-amp-sortByKey" class="headerlink" title="1.12 sortBy &amp; sortByKey"></a>1.12 sortBy &amp; sortByKey</h3><p>按照键进行排序：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list01 = <span class="type">List</span>((<span class="number">100</span>, <span class="string">"hadoop"</span>), (<span class="number">90</span>, <span class="string">"spark"</span>), (<span class="number">120</span>, <span class="string">"storm"</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list01).sortByKey(ascending = <span class="literal">false</span>).foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">(<span class="number">120</span>,storm)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">(<span class="number">90</span>,spark)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">(<span class="number">100</span>,hadoop)</span></pre></td></tr></table></figure>

<p>按照指定元素进行排序：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list02 = <span class="type">List</span>((<span class="string">"hadoop"</span>,<span class="number">100</span>), (<span class="string">"spark"</span>,<span class="number">90</span>), (<span class="string">"storm"</span>,<span class="number">120</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list02).sortBy(x=&gt;x._2,ascending=<span class="literal">false</span>).foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">(storm,<span class="number">120</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">(hadoop,<span class="number">100</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">(spark,<span class="number">90</span>)</span></pre></td></tr></table></figure>

<h3 id="1-13-join"><a href="#1-13-join" class="headerlink" title="1.13 join"></a>1.13 join</h3><p>在一个 (K, V) 和 (K, W) 类型的 Dataset 上调用时，返回一个 (K, (V, W)) 的 Dataset，等价于内连接操作。如果想要执行外连接，可以使用 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 等算子。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list01 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"student01"</span>), (<span class="number">2</span>, <span class="string">"student02"</span>), (<span class="number">3</span>, <span class="string">"student03"</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list02 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"teacher01"</span>), (<span class="number">2</span>, <span class="string">"teacher02"</span>), (<span class="number">3</span>, <span class="string">"teacher03"</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list01).join(sc.parallelize(list02)).foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>,(student01,teacher01))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">(<span class="number">3</span>,(student03,teacher03))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">(<span class="number">2</span>,(student02,teacher02))</span></pre></td></tr></table></figure>

<h3 id="1-14-cogroup"><a href="#1-14-cogroup" class="headerlink" title="1.14 cogroup"></a>1.14 cogroup</h3><p>在一个 (K, V) 对的 Dataset 上调用时，返回多个类型为 (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) 的元组所组成的 Dataset。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list01 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"a"</span>),(<span class="number">1</span>, <span class="string">"a"</span>), (<span class="number">2</span>, <span class="string">"b"</span>), (<span class="number">3</span>, <span class="string">"e"</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list02 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"A"</span>), (<span class="number">2</span>, <span class="string">"B"</span>), (<span class="number">3</span>, <span class="string">"E"</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list03 = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"[ab]"</span>), (<span class="number">2</span>, <span class="string">"[bB]"</span>), (<span class="number">3</span>, <span class="string">"eE"</span>),(<span class="number">3</span>, <span class="string">"eE"</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list01).cogroup(sc.parallelize(list02),sc.parallelize(list03)).foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出： 同一个 RDD 中的元素先按照 key 进行分组，然后再对不同 RDD 中的元素按照 key 进行分组</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>,(<span class="type">CompactBuffer</span>(a, a),<span class="type">CompactBuffer</span>(<span class="type">A</span>),<span class="type">CompactBuffer</span>([ab])))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">(<span class="number">3</span>,(<span class="type">CompactBuffer</span>(e),<span class="type">CompactBuffer</span>(<span class="type">E</span>),<span class="type">CompactBuffer</span>(eE, eE)))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">(<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="type">B</span>),<span class="type">CompactBuffer</span>([bB])))</span></pre></td></tr></table></figure>

<h3 id="1-15-cartesian"><a href="#1-15-cartesian" class="headerlink" title="1.15 cartesian"></a>1.15 cartesian</h3><p>计算笛卡尔积：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="string">"A"</span>, <span class="string">"B"</span>, <span class="string">"C"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list1).cartesian(sc.parallelize(list2)).foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//输出笛卡尔积</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">(<span class="type">A</span>,<span class="number">1</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">(<span class="type">A</span>,<span class="number">2</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">(<span class="type">A</span>,<span class="number">3</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">(<span class="type">B</span>,<span class="number">1</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">(<span class="type">B</span>,<span class="number">2</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">(<span class="type">B</span>,<span class="number">3</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">(<span class="type">C</span>,<span class="number">1</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">(<span class="type">C</span>,<span class="number">2</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">(<span class="type">C</span>,<span class="number">3</span>)</span></pre></td></tr></table></figure>

<h3 id="1-16-aggregateByKey"><a href="#1-16-aggregateByKey" class="headerlink" title="1.16 aggregateByKey"></a>1.16 aggregateByKey</h3><p>当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和 zeroValue 聚合每个键的值。与 <code>groupByKey</code> 类似，reduce 任务的数量可通过第二个参数 <code>numPartitions</code> 进行配置。示例如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 为了清晰，以下所有参数均使用具名传参</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">"hadoop"</span>, <span class="number">3</span>), (<span class="string">"hadoop"</span>, <span class="number">2</span>), (<span class="string">"spark"</span>, <span class="number">4</span>), (<span class="string">"spark"</span>, <span class="number">3</span>), (<span class="string">"storm"</span>, <span class="number">6</span>), (<span class="string">"storm"</span>, <span class="number">8</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list,numSlices = <span class="number">2</span>).aggregateByKey(zeroValue = <span class="number">0</span>,numPartitions = <span class="number">3</span>)(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">      seqOp = math.max(_, _),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">      combOp = _ + _</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    ).collect.foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//输出结果：</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">(hadoop,<span class="number">3</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">(storm,<span class="number">8</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">(spark,<span class="number">7</span>)</span></pre></td></tr></table></figure>

<p>这里使用了 <code>numSlices = 2</code> 指定 aggregateByKey 父操作 parallelize 的分区数量为 2，其执行流程如下：</p>
<div align="center"> <img src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/spark-aggregateByKey.png"/> </div>

<p>基于同样的执行流程，如果 <code>numSlices = 1</code>，则意味着只有输入一个分区，则其最后一步 combOp 相当于是无效的，执行结果为：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="attr">(hadoop,3)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="attr">(storm,8)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="attr">(spark,4)</span></span></pre></td></tr></table></figure>

<p>同样的，如果每个单词对一个分区，即 <code>numSlices = 6</code>，此时相当于求和操作，执行结果为：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="attr">(hadoop,5)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="attr">(storm,14)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="attr">(spark,7)</span></span></pre></td></tr></table></figure>

<p><code>aggregateByKey(zeroValue = 0,numPartitions = 3)</code> 的第二个参数 <code>numPartitions</code> 决定的是输出 RDD 的分区数量，想要验证这个问题，可以对上面代码进行改写，使用 <code>getNumPartitions</code> 方法获取分区数量：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list,numSlices = <span class="number">6</span>).aggregateByKey(zeroValue = <span class="number">0</span>,numPartitions = <span class="number">3</span>)(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  seqOp = math.max(_, _),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  combOp = _ + _</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">).getNumPartitions</span></pre></td></tr></table></figure>

<div align="center"> <img src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/spark-getpartnum.png"/> </div>

<h2 id="二、Action"><a href="#二、Action" class="headerlink" title="二、Action"></a>二、Action</h2><p>Spark 常用的 Action 算子如下：</p>
<table>
<thead>
<tr>
<th>Action（动作）</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody><tr>
<td><strong>reduce</strong>(<em>func</em>)</td>
<td>使用函数<em>func</em>执行归约操作</td>
</tr>
<tr>
<td><strong>collect</strong>()</td>
<td>以一个 array 数组的形式返回 dataset 的所有元素，适用于小结果集。</td>
</tr>
<tr>
<td><strong>count</strong>()</td>
<td>返回 dataset 中元素的个数。</td>
</tr>
<tr>
<td><strong>first</strong>()</td>
<td>返回 dataset 中的第一个元素，等价于 take(1)。</td>
</tr>
<tr>
<td><strong>take</strong>(<em>n</em>)</td>
<td>将数据集中的前 <em>n</em> 个元素作为一个 array 数组返回。</td>
</tr>
<tr>
<td><strong>takeSample</strong>(<em>withReplacement</em>, <em>num</em>, [<em>seed</em>])</td>
<td>对一个 dataset 进行随机抽样</td>
</tr>
<tr>
<td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td>
<td>按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 <em>n</em> 个元素。只适用于小结果集，因为所有数据都会被加载到驱动程序的内存中进行排序。</td>
</tr>
<tr>
<td><strong>saveAsTextFile</strong>(<em>path</em>)</td>
<td>将 dataset 中的元素以文本文件的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。Spark 将对每个元素调用 toString 方法，将元素转换为文本文件中的一行记录。</td>
</tr>
<tr>
<td><strong>saveAsSequenceFile</strong>(<em>path</em>)</td>
<td>将 dataset 中的元素以 Hadoop SequenceFile 的形式写入到本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。该操作要求 RDD 中的元素需要实现 Hadoop 的 Writable 接口。对于 Scala 语言而言，它可以将 Spark 中的基本数据类型自动隐式转换为对应 Writable 类型。(目前仅支持 Java and Scala)</td>
</tr>
<tr>
<td><strong>saveAsObjectFile</strong>(<em>path</em>)</td>
<td>使用 Java 序列化后存储，可以使用 <code>SparkContext.objectFile()</code> 进行加载。(目前仅支持 Java and Scala)</td>
</tr>
<tr>
<td><strong>countByKey</strong>()</td>
<td>计算每个键出现的次数。</td>
</tr>
<tr>
<td><strong>foreach</strong>(<em>func</em>)</td>
<td>遍历 RDD 中每个元素，并对其执行<em>fun</em>函数</td>
</tr>
</tbody></table>
<h3 id="2-1-reduce"><a href="#2-1-reduce" class="headerlink" title="2.1 reduce"></a>2.1 reduce</h3><p>使用函数<em>func</em>执行归约操作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"> <span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list).reduce((x, y) =&gt; x + y)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list).reduce(_ + _)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出 15</span></span></pre></td></tr></table></figure>

<h3 id="2-2-takeOrdered"><a href="#2-2-takeOrdered" class="headerlink" title="2.2 takeOrdered"></a>2.2 takeOrdered</h3><p>按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 <em>n</em> 个元素。需要注意的是 <code>takeOrdered</code> 使用隐式参数进行隐式转换，以下为其源码。所以在使用自定义排序时，需要继承 <code>Ordering[T]</code> 实现自定义比较器，然后将其作为隐式参数引入。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  .........</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>

<p>自定义规则排序：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 继承 Ordering[T],实现自定义比较器，按照 value 值的长度进行排序</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomOrdering</span> <span class="keyword">extends</span> <span class="title">Ordering</span>[(<span class="type">Int</span>, <span class="type">String</span>)] </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: (<span class="type">Int</span>, <span class="type">String</span>), y: (<span class="type">Int</span>, <span class="type">String</span>)): <span class="type">Int</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    = <span class="keyword">if</span> (x._2.length &gt; y._2.length) <span class="number">1</span> <span class="keyword">else</span> <span class="number">-1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="number">1</span>, <span class="string">"hadoop"</span>), (<span class="number">1</span>, <span class="string">"storm"</span>), (<span class="number">1</span>, <span class="string">"azkaban"</span>), (<span class="number">1</span>, <span class="string">"hive"</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//  引入隐式默认值</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="keyword">val</span> implicitOrdering = <span class="keyword">new</span> <span class="type">CustomOrdering</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list).takeOrdered(<span class="number">5</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出： Array((1,hive), (1,storm), (1,hadoop), (1,azkaban)</span></span></pre></td></tr></table></figure>

<h3 id="2-3-countByKey"><a href="#2-3-countByKey" class="headerlink" title="2.3 countByKey"></a>2.3 countByKey</h3><p>计算每个键出现的次数：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">"hadoop"</span>, <span class="number">10</span>), (<span class="string">"hadoop"</span>, <span class="number">10</span>), (<span class="string">"storm"</span>, <span class="number">3</span>), (<span class="string">"storm"</span>, <span class="number">3</span>), (<span class="string">"azkaban"</span>, <span class="number">1</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list).countByKey()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出： Map(hadoop -&gt; 2, storm -&gt; 2, azkaban -&gt; 1)</span></span></pre></td></tr></table></figure>

<h3 id="2-4-saveAsTextFile"><a href="#2-4-saveAsTextFile" class="headerlink" title="2.4 saveAsTextFile"></a>2.4 saveAsTextFile</h3><p>将 dataset 中的元素以文本文件的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。Spark 将对每个元素调用 toString 方法，将元素转换为文本文件中的一行记录。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>((<span class="string">"hadoop"</span>, <span class="number">10</span>), (<span class="string">"hadoop"</span>, <span class="number">10</span>), (<span class="string">"storm"</span>, <span class="number">3</span>), (<span class="string">"storm"</span>, <span class="number">3</span>), (<span class="string">"azkaban"</span>, <span class="number">1</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list).saveAsTextFile(<span class="string">"/usr/file/temp"</span>)</span></pre></td></tr></table></figure>





<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-programming-guide" target="_blank" rel="noopener">RDD Programming Guide</a></p>

        </div>
      </div>
    </div>
  </div>
</article>



    <!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <p class="copyright text-muted">
          Theme By <a target="_blank" href="https://github.com/levblanc">Levblanc.</a>
          Inspired By <a target="_blank" href="https://github.com/klugjo/hexo-theme-clean-blog">Clean Blog.</a>
        <p class="copyright text-muted">
          Powered By <a target="_blank" href="https://hexo.io/">Hexo.</a>
        </p>
      </div>
    </div>
  </div>
</footer>


    <!-- After Footer Scripts -->
<script src="/js/highlight.pack.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function(event) {
    var codeBlocks = Array.prototype.slice.call(document.getElementsByTagName('pre'))
    codeBlocks.forEach(function(block, index) {
      hljs.highlightBlock(block);
    });
  });
</script>

  </body>
</html>

