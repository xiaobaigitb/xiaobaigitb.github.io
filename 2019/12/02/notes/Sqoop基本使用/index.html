<!DOCTYPE html>
<html>
  <!-- Html Head Tag-->
  <head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="Piece One">
  <!-- Open Graph Data -->
  <meta property="og:title" content="notes/Sqoop基本使用"/>
  <meta property="og:description" content="爱吃鱼的Blog" />
  <meta property="og:site_name" content="TLUYZ&#39;Blog"/>
  <meta property="og:type" content="article" />
  <meta property="og:image" content="http://yoursite.com"/>
  
    <link rel="alternate" href="/atom.xml" title="TLUYZ&#39;Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  

  <!-- Site Title -->
  <title>TLUYZ'Blog</title>

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <!-- Custom CSS -->
  
  <link rel="stylesheet" href="/css/style.light.css">

  <!-- Google Analytics -->
  

</head>

  <body>
    <!-- Page Header -->


<header class="site-header header-background" style="background-image: url(/img/default-banner-dark.jpg)">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="page-title with-background-image">
          <p class="title">notes/Sqoop基本使用</p>
          <p class="subtitle"></p>
        </div>
        <div class="site-menu with-background-image">
          <ul>
            
              <li>
                <a href="/">
                  
                  Home
                  
                </a>
              </li>
            
              <li>
                <a href="/archives">
                  
                  Archives
                  
                </a>
              </li>
            
              <li>
                <a href="https://github.com/xiaobaigitb" target="_blank" rel="noopener">
                  
                  Github
                  
                </a>
              </li>
            
              <li>
                <a href="mailto:<your-email-address>">
                  
                  Email
                  
                </a>
              </li>
            
          </ul>
        </div>
      </div>
    </div>
  </div>
</header>

<article>
  <div class="container typo">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-info text-muted">
          
            <!-- Author -->
            <span class="author info">By Piece One</span>
          
          <!-- Date -->
          <span class="date-time info">On
            <span class="date">2019-12-02</span>
            <span class="time">22:05:46</span>
          </span>
          
        </div>
        <!-- Tags -->
        
        <!-- Post Main Content -->
        <div class="post-content">
          <h1 id="Sqoop基本使用"><a href="#Sqoop基本使用" class="headerlink" title="Sqoop基本使用"></a>Sqoop基本使用</h1><nav>
<a href="#一Sqoop-基本命令">一、Sqoop 基本命令</a><br/>
<a href="#二Sqoop-与-MySQL">二、Sqoop 与 MySQL</a><br/>
<a href="#三Sqoop-与-HDFS">三、Sqoop 与 HDFS</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#31-MySQL数据导入到HDFS">3.1 MySQL数据导入到HDFS</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#32-HDFS数据导出到MySQL">3.2 HDFS数据导出到MySQL</a><br/>
<a href="#四Sqoop-与-Hive">四、Sqoop 与 Hive</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#41-MySQL数据导入到Hive">4.1 MySQL数据导入到Hive</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#42-Hive-导出数据到MySQL">4.2 Hive 导出数据到MySQL</a><br/>
<a href="#五Sqoop-与-HBase">五、Sqoop 与 HBase</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#51-MySQL导入数据到HBase">5.1 MySQL导入数据到HBase</a><br/>
<a href="#六全库导出">六、全库导出</a><br/>
<a href="#七Sqoop-数据过滤">七、Sqoop 数据过滤</a><br/>
<a href="#八类型支持">八、类型支持</a><br/>
</nav>


<h2 id="一、Sqoop-基本命令"><a href="#一、Sqoop-基本命令" class="headerlink" title="一、Sqoop 基本命令"></a>一、Sqoop 基本命令</h2><h3 id="1-查看所有命令"><a href="#1-查看所有命令" class="headerlink" title="1. 查看所有命令"></a>1. 查看所有命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> sqoop <span class="built_in">help</span></span></span></pre></td></tr></table></figure>

<div align="center"> <img  src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/sqoop-help.png"/> </div>

<br/>

<h3 id="2-查看某条命令的具体使用方法"><a href="#2-查看某条命令的具体使用方法" class="headerlink" title="2. 查看某条命令的具体使用方法"></a>2. 查看某条命令的具体使用方法</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> sqoop <span class="built_in">help</span> 命令名</span></span></pre></td></tr></table></figure>



<h2 id="二、Sqoop-与-MySQL"><a href="#二、Sqoop-与-MySQL" class="headerlink" title="二、Sqoop 与 MySQL"></a>二、Sqoop 与 MySQL</h2><h3 id="1-查询MySQL所有数据库"><a href="#1-查询MySQL所有数据库" class="headerlink" title="1. 查询MySQL所有数据库"></a>1. 查询MySQL所有数据库</h3><p>通常用于 Sqoop 与 MySQL 连通测试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">sqoop list-databases \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/ \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password root</span></pre></td></tr></table></figure>

<div align="center"> <img  src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/sqoop-list-databases.png"/> </div>

<br/>

<h3 id="2-查询指定数据库中所有数据表"><a href="#2-查询指定数据库中所有数据表" class="headerlink" title="2. 查询指定数据库中所有数据表"></a>2. 查询指定数据库中所有数据表</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">sqoop list-tables \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/mysql \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password root</span></pre></td></tr></table></figure>



<h2 id="三、Sqoop-与-HDFS"><a href="#三、Sqoop-与-HDFS" class="headerlink" title="三、Sqoop 与 HDFS"></a>三、Sqoop 与 HDFS</h2><h3 id="3-1-MySQL数据导入到HDFS"><a href="#3-1-MySQL数据导入到HDFS" class="headerlink" title="3.1 MySQL数据导入到HDFS"></a>3.1 MySQL数据导入到HDFS</h3><h4 id="1-导入命令"><a href="#1-导入命令" class="headerlink" title="1. 导入命令"></a>1. 导入命令</h4><p>示例：导出 MySQL 数据库中的 <code>help_keyword</code> 表到 HDFS 的 <code>/sqoop</code> 目录下，如果导入目录存在则先删除再导入，使用 3 个 <code>map tasks</code> 并行导入。</p>
<blockquote>
<p>注：help_keyword 是 MySQL 内置的一张字典表，之后的示例均使用这张表。</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">--connect jdbc:mysql://hadoop001:3306/mysql \     </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">--username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">--password root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">--table help_keyword \           # 待导入的表</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">--delete-target-dir \            # 目标目录存在则先删除</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">--target-dir /sqoop \            # 导入的目标目录</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">--fields-terminated-by '\t'  \   # 指定导出数据的分隔符</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">-m 3                             # 指定并行执行的 map tasks 数量</span></pre></td></tr></table></figure>

<p>日志输出如下，可以看到输入数据被平均 <code>split</code> 为三份，分别由三个 <code>map task</code> 进行处理。数据默认以表的主键列作为拆分依据，如果你的表没有主键，有以下两种方案：</p>
<ul>
<li>添加 <code>-- autoreset-to-one-mapper</code> 参数，代表只启动一个 <code>map task</code>，即不并行执行；</li>
<li>若仍希望并行执行，则可以使用 <code>--split-by &lt;column-name&gt;</code> 指明拆分数据的参考列。</li>
</ul>
<div align="center"> <img  src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/sqoop-map-task.png"/> </div>

<h4 id="2-导入验证"><a href="#2-导入验证" class="headerlink" title="2. 导入验证"></a>2. 导入验证</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看导入后的目录</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">hadoop fs -ls  -R /sqoop</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看导入内容</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">hadoop fs -text  /sqoop/part-m-00000</span></pre></td></tr></table></figure>

<p>查看 HDFS 导入目录,可以看到表中数据被分为 3 部分进行存储，这是由指定的并行度决定的。</p>
<div align="center"> <img  src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/sqoop_hdfs_ls.png"/> </div>

<br/>

<h3 id="3-2-HDFS数据导出到MySQL"><a href="#3-2-HDFS数据导出到MySQL" class="headerlink" title="3.2 HDFS数据导出到MySQL"></a>3.2 HDFS数据导出到MySQL</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">sqoop export  \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    --connect jdbc:mysql://hadoop001:3306/mysql \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    --username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    --password root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    --table help_keyword_from_hdfs \        # 导出数据存储在 MySQL 的 help_keyword_from_hdf 的表中</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    --export-dir /sqoop  \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    --input-fields-terminated-by '\t'\</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    --m 3</span></pre></td></tr></table></figure>

<p>表必须预先创建，建表语句如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> help_keyword_from_hdfs <span class="keyword">LIKE</span> help_keyword ;</span></pre></td></tr></table></figure>



<h2 id="四、Sqoop-与-Hive"><a href="#四、Sqoop-与-Hive" class="headerlink" title="四、Sqoop 与 Hive"></a>四、Sqoop 与 Hive</h2><h3 id="4-1-MySQL数据导入到Hive"><a href="#4-1-MySQL数据导入到Hive" class="headerlink" title="4.1 MySQL数据导入到Hive"></a>4.1 MySQL数据导入到Hive</h3><p>Sqoop 导入数据到 Hive 是通过先将数据导入到 HDFS 上的临时目录，然后再将数据从 HDFS 上 <code>Load</code> 到 Hive 中，最后将临时目录删除。可以使用 <code>target-dir</code> 来指定临时目录。</p>
<h4 id="1-导入命令-1"><a href="#1-导入命令-1" class="headerlink" title="1. 导入命令"></a>1. 导入命令</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  --connect jdbc:mysql://hadoop001:3306/mysql \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  --username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  --password root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  --table help_keyword \        # 待导入的表     </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  --delete-target-dir \         # 如果临时目录存在删除</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">  --target-dir /sqoop_hive  \   # 临时目录位置</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">  --hive-database sqoop_test \  # 导入到 Hive 的 sqoop_test 数据库，数据库需要预先创建。不指定则默认为 default 库</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">  --hive-import \               # 导入到 Hive</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">  --hive-overwrite \            # 如果 Hive 表中有数据则覆盖，这会清除表中原有的数据，然后再写入</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">  -m 3                          # 并行度</span></pre></td></tr></table></figure>

<p>导入到 Hive 中的 <code>sqoop_test</code> 数据库需要预先创建，不指定则默认使用 Hive 中的 <code>default</code> 库。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看 hive 中的所有数据库</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash">  SHOW DATABASES;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建 sqoop_test 数据库</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash">  CREATE DATABASE sqoop_test;</span></span></pre></td></tr></table></figure>

<h4 id="2-导入验证-1"><a href="#2-导入验证-1" class="headerlink" title="2. 导入验证"></a>2. 导入验证</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看 sqoop_test 数据库的所有表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="meta"> hive&gt;</span><span class="bash">  SHOW  TABLES  IN  sqoop_test;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看表中数据</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="meta"> hive&gt;</span><span class="bash"> SELECT * FROM sqoop_test.help_keyword;</span></span></pre></td></tr></table></figure>

<div align="center"> <img  src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/sqoop_hive_tables.png"/> </div>

<h4 id="3-可能出现的问题"><a href="#3-可能出现的问题" class="headerlink" title="3. 可能出现的问题"></a>3. 可能出现的问题</h4><div align="center"> <img  src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/sqoop_hive_error.png"/> </div>

<br/>

<p>如果执行报错 <code>java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf</code>，则需将 Hive 安装目录下 <code>lib</code> 下的 <code>hive-exec-**.jar</code> 放到 sqoop 的 <code>lib</code> 。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 lib]# ll hive-exec-*</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">-rw-r--r--. 1 1106 4001 19632031 11 月 13 21:45 hive-exec-1.1.0-cdh5.15.2.jar</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">[root@hadoop001 lib]# cp hive-exec-1.1.0-cdh5.15.2.jar  $&#123;SQOOP_HOME&#125;/lib</span></pre></td></tr></table></figure>

<br/>

<h3 id="4-2-Hive-导出数据到MySQL"><a href="#4-2-Hive-导出数据到MySQL" class="headerlink" title="4.2 Hive 导出数据到MySQL"></a>4.2 Hive 导出数据到MySQL</h3><p>由于 Hive 的数据是存储在 HDFS 上的，所以 Hive 导入数据到 MySQL，实际上就是 HDFS 导入数据到 MySQL。</p>
<h4 id="1-查看Hive表在HDFS的存储位置"><a href="#1-查看Hive表在HDFS的存储位置" class="headerlink" title="1. 查看Hive表在HDFS的存储位置"></a>1. 查看Hive表在HDFS的存储位置</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 进入对应的数据库</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> use sqoop_test;</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看表信息</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> desc formatted help_keyword;</span></span></pre></td></tr></table></figure>

<p><code>Location</code> 属性为其存储位置：</p>
<div align="center"> <img  src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/sqoop-hive-location.png"/> </div>

<p>这里可以查看一下这个目录，文件结构如下：</p>
<div align="center"> <img  src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/sqoop-hive-hdfs.png"/> </div>

<h4 id="3-2-执行导出命令"><a href="#3-2-执行导出命令" class="headerlink" title="3.2 执行导出命令"></a>3.2 执行导出命令</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">sqoop export  \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    --connect jdbc:mysql://hadoop001:3306/mysql \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    --username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    --password root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    --table help_keyword_from_hive \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    --export-dir /user/hive/warehouse/sqoop_test.db/help_keyword  \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    -input-fields-terminated-by '\001' \             # 需要注意的是 hive 中默认的分隔符为 \001</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    --m 3</span></pre></td></tr></table></figure>
<p>MySQL 中的表需要预先创建：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> help_keyword_from_hive <span class="keyword">LIKE</span> help_keyword ;</span></pre></td></tr></table></figure>



<h2 id="五、Sqoop-与-HBase"><a href="#五、Sqoop-与-HBase" class="headerlink" title="五、Sqoop 与 HBase"></a>五、Sqoop 与 HBase</h2><blockquote>
<p>本小节只讲解从 RDBMS 导入数据到 HBase，因为暂时没有命令能够从 HBase 直接导出数据到 RDBMS。</p>
</blockquote>
<h3 id="5-1-MySQL导入数据到HBase"><a href="#5-1-MySQL导入数据到HBase" class="headerlink" title="5.1 MySQL导入数据到HBase"></a>5.1 MySQL导入数据到HBase</h3><h4 id="1-导入数据"><a href="#1-导入数据" class="headerlink" title="1. 导入数据"></a>1. 导入数据</h4><p>将 <code>help_keyword</code> 表中数据导入到 HBase 上的 <code>help_keyword_hbase</code> 表中，使用原表的主键 <code>help_keyword_id</code> 作为 <code>RowKey</code>，原表的所有列都会在 <code>keywordInfo</code> 列族下，目前只支持全部导入到一个列族下，不支持分别指定列族。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    --connect jdbc:mysql://hadoop001:3306/mysql \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    --username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    --password root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    --table help_keyword \              # 待导入的表</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    --hbase-table help_keyword_hbase \  # hbase 表名称，表需要预先创建</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    --column-family keywordInfo \       # 所有列导入到 keywordInfo 列族下 </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    --hbase-row-key help_keyword_id     # 使用原表的 help_keyword_id 作为 RowKey</span></pre></td></tr></table></figure>

<p>导入的 HBase 表需要预先创建：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看所有表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> list</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> create <span class="string">'help_keyword_hbase'</span>, <span class="string">'keywordInfo'</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看表信息</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> desc <span class="string">'help_keyword_hbase'</span></span></span></pre></td></tr></table></figure>

<h4 id="2-导入验证-2"><a href="#2-导入验证-2" class="headerlink" title="2. 导入验证"></a>2. 导入验证</h4><p>使用 <code>scan</code> 查看表数据：</p>
<div align="center"> <img  src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/sqoop_hbase.png"/> </div>





<h2 id="六、全库导出"><a href="#六、全库导出" class="headerlink" title="六、全库导出"></a>六、全库导出</h2><p>Sqoop 支持通过 <code>import-all-tables</code> 命令进行全库导出到 HDFS/Hive，但需要注意有以下两个限制：</p>
<ul>
<li>所有表必须有主键；或者使用 <code>--autoreset-to-one-mapper</code>，代表只启动一个 <code>map task</code>;</li>
<li>你不能使用非默认的分割列，也不能通过 WHERE 子句添加任何限制。</li>
</ul>
<blockquote>
<p>第二点解释得比较拗口，这里列出官方原本的说明：</p>
<ul>
<li>You must not intend to use non-default splitting column, nor impose any conditions via a <code>WHERE</code> clause.</li>
</ul>
</blockquote>
<p>全库导出到 HDFS：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">sqoop import-all-tables \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    --connect jdbc:mysql://hadoop001:3306/数据库名 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    --username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    --password root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    --warehouse-dir  /sqoop_all \     # 每个表会单独导出到一个目录，需要用此参数指明所有目录的父目录</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    --fields-terminated-by '\t'  \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    -m 3</span></pre></td></tr></table></figure>

<p>全库导出到 Hive：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">sqoop import-all-tables -Dorg.apache.sqoop.splitter.allow_text_splitter=true \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  --connect jdbc:mysql://hadoop001:3306/数据库名 \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  --username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  --password root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  --hive-database sqoop_test \         # 导出到 Hive 对应的库   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  --hive-import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">  --hive-overwrite \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">  -m 3</span></pre></td></tr></table></figure>



<h2 id="七、Sqoop-数据过滤"><a href="#七、Sqoop-数据过滤" class="headerlink" title="七、Sqoop 数据过滤"></a>七、Sqoop 数据过滤</h2><h3 id="7-1-query参数"><a href="#7-1-query参数" class="headerlink" title="7.1 query参数"></a>7.1 query参数</h3><p>Sqoop 支持使用 <code>query</code> 参数定义查询 SQL，从而可以导出任何想要的结果集。使用示例如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">  --connect jdbc:mysql://hadoop001:3306/mysql \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">  --username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">  --password root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">  --query 'select * from help_keyword where  $CONDITIONS and  help_keyword_id &lt; 50' \  </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">  --delete-target-dir \            </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">  --target-dir /sqoop_hive  \ </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">  --hive-database sqoop_test \           # 指定导入目标数据库 不指定则默认使用 Hive 中的 default 库</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">  --hive-table filter_help_keyword \     # 指定导入目标表</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">  --split-by help_keyword_id \           # 指定用于 split 的列      </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">  --hive-import \                        # 导入到 Hive</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">  --hive-overwrite \                     、</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">  -m 3</span></pre></td></tr></table></figure>

<p>在使用 <code>query</code> 进行数据过滤时，需要注意以下三点：</p>
<ul>
<li>必须用 <code>--hive-table</code> 指明目标表；</li>
<li>如果并行度 <code>-m</code> 不为 1 或者没有指定 <code>--autoreset-to-one-mapper</code>，则需要用 <code>--split-by</code> 指明参考列；</li>
<li>SQL 的 <code>where</code> 字句必须包含 <code>$CONDITIONS</code>，这是固定写法，作用是动态替换。</li>
</ul>
<h3 id="7-2-增量导入"><a href="#7-2-增量导入" class="headerlink" title="7.2 增量导入"></a>7.2 增量导入</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">sqoop import \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    --connect jdbc:mysql://hadoop001:3306/mysql \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    --username root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    --password root \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    --table help_keyword \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    --target-dir /sqoop_hive  \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    --hive-database sqoop_test \         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    --incremental  append  \             # 指明模式</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    --check-column  help_keyword_id \    # 指明用于增量导入的参考列</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">    --last-value 300  \                  # 指定参考列上次导入的最大值</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    --hive-import \   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    -m 3</span></pre></td></tr></table></figure>

<p><code>incremental</code> 参数有以下两个可选的选项：</p>
<ul>
<li><strong>append</strong>：要求参考列的值必须是递增的，所有大于 <code>last-value</code> 的值都会被导入；</li>
<li><strong>lastmodified</strong>：要求参考列的值必须是 <code>timestamp</code> 类型，且插入数据时候要在参考列插入当前时间戳，更新数据时也要更新参考列的时间戳，所有时间晚于 <code>last-value</code> 的数据都会被导入。</li>
</ul>
<p>通过上面的解释我们可以看出来，其实 Sqoop 的增量导入并没有太多神器的地方，就是依靠维护的参考列来判断哪些是增量数据。当然我们也可以使用上面介绍的 <code>query</code> 参数来进行手动的增量导出，这样反而更加灵活。</p>
<h2 id="八、类型支持"><a href="#八、类型支持" class="headerlink" title="八、类型支持"></a>八、类型支持</h2><p>Sqoop 默认支持数据库的大多数字段类型，但是某些特殊类型是不支持的。遇到不支持的类型，程序会抛出异常 <code>Hive does not support the SQL type for column xxx</code> 异常，此时可以通过下面两个参数进行强制类型转换：</p>
<ul>
<li><strong>–map-column-java&lt;mapping&gt;</strong>   ：重写 SQL 到 Java 类型的映射；</li>
<li><strong>–map-column-hive &lt;mapping&gt;</strong> ： 重写 Hive 到 Java 类型的映射。</li>
</ul>
<p>示例如下，将原先 <code>id</code> 字段强制转为 String 类型，<code>value</code> 字段强制转为 Integer 类型：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">$ sqoop import ... --map-column-java id&#x3D;String,value&#x3D;Integer</span></pre></td></tr></table></figure>





<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html" target="_blank" rel="noopener">Sqoop User Guide (v1.4.7)</a></p>

        </div>
      </div>
    </div>
  </div>
</article>



    <!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <p class="copyright text-muted">
          Theme By <a target="_blank" href="https://github.com/levblanc">Levblanc.</a>
          Inspired By <a target="_blank" href="https://github.com/klugjo/hexo-theme-clean-blog">Clean Blog.</a>
        <p class="copyright text-muted">
          Powered By <a target="_blank" href="https://hexo.io/">Hexo.</a>
        </p>
      </div>
    </div>
  </div>
</footer>


    <!-- After Footer Scripts -->
<script src="/js/highlight.pack.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function(event) {
    var codeBlocks = Array.prototype.slice.call(document.getElementsByTagName('pre'))
    codeBlocks.forEach(function(block, index) {
      hljs.highlightBlock(block);
    });
  });
</script>

  </body>
</html>

