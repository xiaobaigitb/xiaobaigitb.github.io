<!DOCTYPE html>
<html>
  <!-- Html Head Tag-->
  <head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="Piece One">
  <!-- Open Graph Data -->
  <meta property="og:title" content="notes/Spark_RDD"/>
  <meta property="og:description" content="爱吃鱼的Blog" />
  <meta property="og:site_name" content="TLUYZ&#39;Blog"/>
  <meta property="og:type" content="article" />
  <meta property="og:image" content="http://yoursite.com"/>
  
    <link rel="alternate" href="/atom.xml" title="TLUYZ&#39;Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  

  <!-- Site Title -->
  <title>TLUYZ'Blog</title>

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <!-- Custom CSS -->
  
  <link rel="stylesheet" href="/css/style.light.css">

  <!-- Google Analytics -->
  

</head>

  <body>
    <!-- Page Header -->


<header class="site-header header-background" style="background-image: url(/img/default-banner-dark.jpg)">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="page-title with-background-image">
          <p class="title">notes/Spark_RDD</p>
          <p class="subtitle"></p>
        </div>
        <div class="site-menu with-background-image">
          <ul>
            
              <li>
                <a href="/">
                  
                  Home
                  
                </a>
              </li>
            
              <li>
                <a href="/archives">
                  
                  Archives
                  
                </a>
              </li>
            
              <li>
                <a href="https://github.com/xiaobaigitb" target="_blank" rel="noopener">
                  
                  Github
                  
                </a>
              </li>
            
              <li>
                <a href="mailto:<your-email-address>">
                  
                  Email
                  
                </a>
              </li>
            
          </ul>
        </div>
      </div>
    </div>
  </div>
</header>

<article>
  <div class="container typo">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-info text-muted">
          
            <!-- Author -->
            <span class="author info">By Piece One</span>
          
          <!-- Date -->
          <span class="date-time info">On
            <span class="date">2019-12-02</span>
            <span class="time">22:05:46</span>
          </span>
          
        </div>
        <!-- Tags -->
        
        <!-- Post Main Content -->
        <div class="post-content">
          <h1 id="弹性式数据集RDDs"><a href="#弹性式数据集RDDs" class="headerlink" title="弹性式数据集RDDs"></a>弹性式数据集RDDs</h1><nav>
<a href="#一RDD简介">一、RDD简介</a><br/>
<a href="#二创建RDD">二、创建RDD</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#21-由现有集合创建">2.1 由现有集合创建</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#22-引用外部存储系统中的数据集">2.2 引用外部存储系统中的数据集</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#23-textFile--wholeTextFiles">2.3 textFile & wholeTextFiles</a><br/>
<a href="#三操作RDD">三、操作RDD</a><br/>
<a href="#四缓存RDD">四、缓存RDD</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#41-缓存级别">4.1 缓存级别</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#42-使用缓存">4.2 使用缓存</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#43-移除缓存">4.3 移除缓存</a><br/>
<a href="#五理解shuffle">五、理解shuffle</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#51-shuffle介绍">5.1 shuffle介绍</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#52-Shuffle的影响">5.2 Shuffle的影响</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#53-导致Shuffle的操作">5.3 导致Shuffle的操作</a><br/>
<a href="#五宽依赖和窄依赖">五、宽依赖和窄依赖</a><br/>
<a href="#六DAG的生成">六、DAG的生成</a><br/>
</nav>

<h2 id="一、RDD简介"><a href="#一、RDD简介" class="headerlink" title="一、RDD简介"></a>一、RDD简介</h2><p><code>RDD</code> 全称为 Resilient Distributed Datasets，是 Spark 最基本的数据抽象，它是只读的、分区记录的集合，支持并行操作，可以由外部数据集或其他 RDD 转换而来，它具有以下特性：</p>
<ul>
<li>一个 RDD 由一个或者多个分区（Partitions）组成。对于 RDD 来说，每个分区会被一个计算任务所处理，用户可以在创建 RDD 时指定其分区个数，如果没有指定，则默认采用程序所分配到的 CPU 的核心数；</li>
<li>RDD 拥有一个用于计算分区的函数 compute；</li>
<li>RDD 会保存彼此间的依赖关系，RDD 的每次转换都会生成一个新的依赖关系，这种 RDD 之间的依赖关系就像流水线一样。在部分分区数据丢失后，可以通过这种依赖关系重新计算丢失的分区数据，而不是对 RDD 的所有分区进行重新计算；</li>
<li>Key-Value 型的 RDD 还拥有 Partitioner(分区器)，用于决定数据被存储在哪个分区中，目前 Spark 中支持 HashPartitioner(按照哈希分区) 和 RangeParationer(按照范围进行分区)；</li>
<li>一个优先位置列表 (可选)，用于存储每个分区的优先位置 (prefered location)。对于一个 HDFS 文件来说，这个列表保存的就是每个分区所在的块的位置，按照“移动数据不如移动计算“的理念，Spark 在进行任务调度的时候，会尽可能的将计算任务分配到其所要处理数据块的存储位置。</li>
</ul>
<p><code>RDD[T]</code> 抽象类的部分相关代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 由子类实现以计算给定分区</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取所有分区</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取所有依赖关系</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取优先位置列表</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 分区器 由子类重写以指定它们的分区方式</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="meta">@transient</span> <span class="keyword">val</span> partitioner: <span class="type">Option</span>[<span class="type">Partitioner</span>] = <span class="type">None</span></span></pre></td></tr></table></figure>



<h2 id="二、创建RDD"><a href="#二、创建RDD" class="headerlink" title="二、创建RDD"></a>二、创建RDD</h2><p>RDD 有两种创建方式，分别介绍如下：</p>
<h3 id="2-1-由现有集合创建"><a href="#2-1-由现有集合创建" class="headerlink" title="2.1 由现有集合创建"></a>2.1 由现有集合创建</h3><p>这里使用 <code>spark-shell</code> 进行测试，启动命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">spark-shell --master local[4]</span></pre></td></tr></table></figure>

<p>启动 <code>spark-shell</code> 后，程序会自动创建应用上下文，相当于执行了下面的 Scala 语句：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Spark shell"</span>).setMaster(<span class="string">"local[4]"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span></pre></td></tr></table></figure>

<p>由现有集合创建 RDD，你可以在创建时指定其分区个数，如果没有指定，则采用程序所分配到的 CPU 的核心数：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 由现有集合创建 RDD,默认分区数为程序所分配到的 CPU 的核心数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sc.parallelize(data) </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 查看分区数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">dataRDD.getNumPartitions</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 明确指定分区数</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sc.parallelize(data,<span class="number">2</span>)</span></pre></td></tr></table></figure>

<p>执行结果如下：</p>
<div align="center"> <img src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/scala-分区数.png"/> </div>

<h3 id="2-2-引用外部存储系统中的数据集"><a href="#2-2-引用外部存储系统中的数据集" class="headerlink" title="2.2 引用外部存储系统中的数据集"></a>2.2 引用外部存储系统中的数据集</h3><p>引用外部存储系统中的数据集，例如本地文件系统，HDFS，HBase 或支持 Hadoop InputFormat 的任何数据源。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fileRDD = sc.textFile(<span class="string">"/usr/file/emp.txt"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取第一行文本</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">fileRDD.take(<span class="number">1</span>)</span></pre></td></tr></table></figure>

<p>使用外部存储系统时需要注意以下两点：</p>
<ul>
<li>如果在集群环境下从本地文件系统读取数据，则要求该文件必须在集群中所有机器上都存在，且路径相同；</li>
<li>支持目录路径，支持压缩文件，支持使用通配符。</li>
</ul>
<h3 id="2-3-textFile-amp-wholeTextFiles"><a href="#2-3-textFile-amp-wholeTextFiles" class="headerlink" title="2.3 textFile &amp; wholeTextFiles"></a>2.3 textFile &amp; wholeTextFiles</h3><p>两者都可以用来读取外部文件，但是返回格式是不同的：</p>
<ul>
<li><strong>textFile</strong>：其返回格式是 <code>RDD[String]</code> ，返回的是就是文件内容，RDD 中每一个元素对应一行数据；</li>
<li><strong>wholeTextFiles</strong>：其返回格式是 <code>RDD[(String, String)]</code>，元组中第一个参数是文件路径，第二个参数是文件内容；</li>
<li>两者都提供第二个参数来控制最小分区数；</li>
<li>从 HDFS 上读取文件时，Spark 会为每个块创建一个分区。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(path: <span class="type">String</span>,minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;...&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wholeTextFiles</span></span>(path: <span class="type">String</span>,minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)]=&#123;..&#125;</span></pre></td></tr></table></figure>



<h2 id="三、操作RDD"><a href="#三、操作RDD" class="headerlink" title="三、操作RDD"></a>三、操作RDD</h2><p>RDD 支持两种类型的操作：<em>transformations<em>（转换，从现有数据集创建新数据集）和 *actions</em>（在数据集上运行计算后将值返回到驱动程序）。RDD 中的所有转换操作都是惰性的，它们只是记住这些转换操作，但不会立即执行，只有遇到 *action</em> 操作后才会真正的进行计算，这类似于函数式编程中的惰性求值。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// map 是一个 transformations 操作，而 foreach 是一个 actions 操作</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">sc.parallelize(list).map(_ * <span class="number">10</span>).foreach(println)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出： 10 20 30</span></span></pre></td></tr></table></figure>



<h2 id="四、缓存RDD"><a href="#四、缓存RDD" class="headerlink" title="四、缓存RDD"></a>四、缓存RDD</h2><h3 id="4-1-缓存级别"><a href="#4-1-缓存级别" class="headerlink" title="4.1 缓存级别"></a>4.1 缓存级别</h3><p>Spark 速度非常快的一个原因是 RDD 支持缓存。成功缓存后，如果之后的操作使用到了该数据集，则直接从缓存中获取。虽然缓存也有丢失的风险，但是由于 RDD 之间的依赖关系，如果某个分区的缓存数据丢失，只需要重新计算该分区即可。</p>
<p>Spark 支持多种缓存级别 ：</p>
<table>
<thead>
<tr>
<th>Storage Level<br/>（存储级别）</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody><tr>
<td><code>MEMORY_ONLY</code></td>
<td>默认的缓存级别，将 RDD 以反序列化的 Java 对象的形式存储在 JVM 中。如果内存空间不够，则部分分区数据将不再缓存。</td>
</tr>
<tr>
<td><code>MEMORY_AND_DISK</code></td>
<td>将 RDD 以反序列化的 Java 对象的形式存储 JVM 中。如果内存空间不够，将未缓存的分区数据存储到磁盘，在需要使用这些分区时从磁盘读取。</td>
</tr>
<tr>
<td><code>MEMORY_ONLY_SER</code><br/></td>
<td>将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式比反序列化对象节省存储空间，但在读取时会增加 CPU 的计算负担。仅支持 Java 和 Scala 。</td>
</tr>
<tr>
<td><code>MEMORY_AND_DISK_SER</code><br/></td>
<td>类似于 <code>MEMORY_ONLY_SER</code>，但是溢出的分区数据会存储到磁盘，而不是在用到它们时重新计算。仅支持 Java 和 Scala。</td>
</tr>
<tr>
<td><code>DISK_ONLY</code></td>
<td>只在磁盘上缓存 RDD</td>
</tr>
<tr>
<td><code>MEMORY_ONLY_2</code>, <br/><code>MEMORY_AND_DISK_2</code>, etc</td>
<td>与上面的对应级别功能相同，但是会为每个分区在集群中的两个节点上建立副本。</td>
</tr>
<tr>
<td><code>OFF_HEAP</code></td>
<td>与 <code>MEMORY_ONLY_SER</code> 类似，但将数据存储在堆外内存中。这需要启用堆外内存。</td>
</tr>
</tbody></table>
<blockquote>
<p>启动堆外内存需要配置两个参数：</p>
<ul>
<li><strong>spark.memory.offHeap.enabled</strong> ：是否开启堆外内存，默认值为 false，需要设置为 true；</li>
<li><strong>spark.memory.offHeap.size</strong> : 堆外内存空间的大小，默认值为 0，需要设置为正值。</li>
</ul>
</blockquote>
<h3 id="4-2-使用缓存"><a href="#4-2-使用缓存" class="headerlink" title="4.2 使用缓存"></a>4.2 使用缓存</h3><p>缓存数据的方法有两个：<code>persist</code> 和 <code>cache</code> 。<code>cache</code> 内部调用的也是 <code>persist</code>，它是 <code>persist</code> 的特殊化形式，等价于 <code>persist(StorageLevel.MEMORY_ONLY)</code>。示例如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">// 所有存储级别均定义在 StorageLevel 对象中</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">fileRDD.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">fileRDD.cache()</span></pre></td></tr></table></figure>

<h3 id="4-3-移除缓存"><a href="#4-3-移除缓存" class="headerlink" title="4.3 移除缓存"></a>4.3 移除缓存</h3><p>Spark 会自动监视每个节点上的缓存使用情况，并按照最近最少使用（LRU）的规则删除旧数据分区。当然，你也可以使用 <code>RDD.unpersist()</code> 方法进行手动删除。</p>
<h2 id="五、理解shuffle"><a href="#五、理解shuffle" class="headerlink" title="五、理解shuffle"></a>五、理解shuffle</h2><h3 id="5-1-shuffle介绍"><a href="#5-1-shuffle介绍" class="headerlink" title="5.1 shuffle介绍"></a>5.1 shuffle介绍</h3><p>在 Spark 中，一个任务对应一个分区，通常不会跨分区操作数据。但如果遇到 <code>reduceByKey</code> 等操作，Spark 必须从所有分区读取数据，并查找所有键的所有值，然后汇总在一起以计算每个键的最终结果 ，这称为 <code>Shuffle</code>。</p>
<div align="center"> <img width="600px" src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/spark-reducebykey.png"/> </div>



<h3 id="5-2-Shuffle的影响"><a href="#5-2-Shuffle的影响" class="headerlink" title="5.2 Shuffle的影响"></a>5.2 Shuffle的影响</h3><p>Shuffle 是一项昂贵的操作，因为它通常会跨节点操作数据，这会涉及磁盘 I/O，网络 I/O，和数据序列化。某些 Shuffle 操作还会消耗大量的堆内存，因为它们使用堆内存来临时存储需要网络传输的数据。Shuffle 还会在磁盘上生成大量中间文件，从 Spark 1.3 开始，这些文件将被保留，直到相应的 RDD 不再使用并进行垃圾回收，这样做是为了避免在计算时重复创建 Shuffle 文件。如果应用程序长期保留对这些 RDD 的引用，则垃圾回收可能在很长一段时间后才会发生，这意味着长时间运行的 Spark 作业可能会占用大量磁盘空间，通常可以使用 <code>spark.local.dir</code> 参数来指定这些临时文件的存储目录。</p>
<h3 id="5-3-导致Shuffle的操作"><a href="#5-3-导致Shuffle的操作" class="headerlink" title="5.3 导致Shuffle的操作"></a>5.3 导致Shuffle的操作</h3><p>由于 Shuffle 操作对性能的影响比较大，所以需要特别注意使用，以下操作都会导致 Shuffle：</p>
<ul>
<li><strong>涉及到重新分区操作</strong>： 如 <code>repartition</code> 和 <code>coalesce</code>；</li>
<li><strong>所有涉及到 ByKey 的操作</strong>：如 <code>groupByKey</code> 和 <code>reduceByKey</code>，但 <code>countByKey</code> 除外；</li>
<li><strong>联结操作</strong>：如 <code>cogroup</code> 和 <code>join</code>。</li>
</ul>
<h2 id="五、宽依赖和窄依赖"><a href="#五、宽依赖和窄依赖" class="headerlink" title="五、宽依赖和窄依赖"></a>五、宽依赖和窄依赖</h2><p>RDD 和它的父 RDD(s) 之间的依赖关系分为两种不同的类型：</p>
<ul>
<li><strong>窄依赖 (narrow dependency)</strong>：父 RDDs 的一个分区最多被子 RDDs 一个分区所依赖；</li>
<li><strong>宽依赖 (wide dependency)</strong>：父 RDDs 的一个分区可以被子 RDDs 的多个子分区所依赖。</li>
</ul>
<p>如下图，每一个方框表示一个 RDD，带有颜色的矩形表示分区：</p>
<div align="center"> <img width="600px" src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/spark-窄依赖和宽依赖.png"/> </div>



<p>区分这两种依赖是非常有用的：</p>
<ul>
<li>首先，窄依赖允许在一个集群节点上以流水线的方式（pipeline）对父分区数据进行计算，例如先执行 map 操作，然后执行 filter 操作。而宽依赖则需要计算好所有父分区的数据，然后再在节点之间进行 Shuffle，这与 MapReduce 类似。</li>
<li>窄依赖能够更有效地进行数据恢复，因为只需重新对丢失分区的父分区进行计算，且不同节点之间可以并行计算；而对于宽依赖而言，如果数据丢失，则需要对所有父分区数据进行计算并再次 Shuffle。</li>
</ul>
<h2 id="六、DAG的生成"><a href="#六、DAG的生成" class="headerlink" title="六、DAG的生成"></a>六、DAG的生成</h2><p>RDD(s) 及其之间的依赖关系组成了 DAG(有向无环图)，DAG 定义了这些 RDD(s) 之间的 Lineage(血统) 关系，通过血统关系，如果一个 RDD 的部分或者全部计算结果丢失了，也可以重新进行计算。那么 Spark 是如何根据 DAG 来生成计算任务呢？主要是根据依赖关系的不同将 DAG 划分为不同的计算阶段 (Stage)：</p>
<ul>
<li>对于窄依赖，由于分区的依赖关系是确定的，其转换操作可以在同一个线程执行，所以可以划分到同一个执行阶段；</li>
<li>对于宽依赖，由于 Shuffle 的存在，只能在父 RDD(s) 被 Shuffle 处理完成后，才能开始接下来的计算，因此遇到宽依赖就需要重新划分阶段。</li>
</ul>
<div align="center"> <img width="600px" height="600px" src="https://github.com/heibaiying/BigData-Notes/blob/master/pictures/spark-DAG.png"/> </div>





<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>张安站 . Spark 技术内幕：深入解析 Spark 内核架构设计与实现原理[M] . 机械工业出版社 . 2015-09-01</li>
<li><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-programming-guide" target="_blank" rel="noopener">RDD Programming Guide</a></li>
<li><a href="http://shiyanjun.cn/archives/744.html" target="_blank" rel="noopener">RDD：基于内存的集群计算容错抽象</a></li>
</ol>

        </div>
      </div>
    </div>
  </div>
</article>



    <!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <p class="copyright text-muted">
          Theme By <a target="_blank" href="https://github.com/levblanc">Levblanc.</a>
          Inspired By <a target="_blank" href="https://github.com/klugjo/hexo-theme-clean-blog">Clean Blog.</a>
        <p class="copyright text-muted">
          Powered By <a target="_blank" href="https://hexo.io/">Hexo.</a>
        </p>
      </div>
    </div>
  </div>
</footer>


    <!-- After Footer Scripts -->
<script src="/js/highlight.pack.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function(event) {
    var codeBlocks = Array.prototype.slice.call(document.getElementsByTagName('pre'))
    codeBlocks.forEach(function(block, index) {
      hljs.highlightBlock(block);
    });
  });
</script>

  </body>
</html>

